{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DO NOT EDIT] Header\n",
    "\n",
    "本笔记本比较所有已实现的数字分类模型。\n",
    "所有代码必须遵循实现指南中定义的项目结构和命名约定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 魔法命令\n",
    "%matplotlib inline\n",
    "\n",
    "# 路径设置 - 导入此模块会自动设置Python路径\n",
    "import path_setup\n",
    "\n",
    "# Constants\n",
    "NOTEBOOK_BASENAME = \"part8_comparison\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(\"../results/figures\", exist_ok=True)\n",
    "os.makedirs(\"../results/metrics\", exist_ok=True)\n",
    "\n",
    "print(\"✓ 环境设置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metrics\n",
    "\n",
    "从所有模型笔记本加载指标进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model notebooks to load metrics from\n",
    "model_notebooks = [\n",
    "    \"part1_logistic_regression\",\n",
    "    \"part2_linear_svm\",\n",
    "    \"part3_knn\",\n",
    "    \"part4_decision_tree\",\n",
    "    \"part5_random_forest\",\n",
    "    \"part6_sgd\",\n",
    "    \"part7_perceptron\"\n",
    "]\n",
    "\n",
    "# Load metrics from each model\n",
    "model_metrics = {}\n",
    "for notebook in model_notebooks:\n",
    "    metrics_path = f\"../results/metrics/{notebook}__metrics.json\"\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, 'r', encoding='utf-8') as f:\n",
    "            metrics = json.load(f)\n",
    "            model_metrics[notebook] = metrics\n",
    "    else:\n",
    "        print(f\"Warning: Metrics file not found for {notebook}\")\n",
    "\n",
    "# Print loaded metrics\n",
    "for model, metrics in model_metrics.items():\n",
    "    print(f\"{model}: Accuracy = {metrics.get('accuracy', 'N/A')}, Macro F1 = {metrics.get('macro_f1', 'N/A')}, Train Time = {metrics.get('train_time_sec', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "为所有模型创建比较可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "model_names = []\n",
    "accuracies = []\n",
    "macro_f1_scores = []\n",
    "train_times = []\n",
    "\n",
    "for notebook, metrics in model_metrics.items():\n",
    "    model_names.append(metrics['model_name'])\n",
    "    accuracies.append(metrics['accuracy'])\n",
    "    macro_f1_scores.append(metrics['macro_f1'])\n",
    "    train_times.append(metrics['train_time_sec'])\n",
    "\n",
    "# ① Accuracy comparison plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(model_names, accuracies, color='steelblue')\n",
    "ax.set_xlabel('Accuracy', fontsize=12)\n",
    "ax.set_ylabel('Model', fontsize=12)\n",
    "ax.set_title('Model Comparison: Accuracy', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "out_png_path = f\"../results/figures/{NOTEBOOK_BASENAME}__accuracy.png\"\n",
    "plt.savefig(out_png_path, dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Accuracy comparison plot saved to {out_png_path}\")\n",
    "\n",
    "# ② Macro F1 comparison plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(model_names, macro_f1_scores, color='seagreen')\n",
    "ax.set_xlabel('Macro F1 Score', fontsize=12)\n",
    "ax.set_ylabel('Model', fontsize=12)\n",
    "ax.set_title('Model Comparison: Macro F1 Score', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "out_png_path = f\"../results/figures/{NOTEBOOK_BASENAME}__macro_f1.png\"\n",
    "plt.savefig(out_png_path, dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Macro F1 comparison plot saved to {out_png_path}\")\n",
    "\n",
    "# ③ Training time comparison plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(model_names, train_times, color='coral')\n",
    "ax.set_xlabel('Training Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Model', fontsize=12)\n",
    "ax.set_title('Model Comparison: Training Time', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "out_png_path = f\"../results/figures/{NOTEBOOK_BASENAME}__train_time.png\"\n",
    "plt.savefig(out_png_path, dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Training time comparison plot saved to {out_png_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "总结比较结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model for each metric\n",
    "best_accuracy_idx = np.argmax(accuracies)\n",
    "best_f1_idx = np.argmax(macro_f1_scores)\n",
    "best_speed_idx = np.argmin(train_times)\n",
    "\n",
    "print(f\"Best accuracy: {model_names[best_accuracy_idx]} ({accuracies[best_accuracy_idx]:.4f})\")\n",
    "print(f\"Best macro F1: {model_names[best_f1_idx]} ({macro_f1_scores[best_f1_idx]:.4f})\")\n",
    "print(f\"Fastest training: {model_names[best_speed_idx]} ({train_times[best_speed_idx]:.4f}s)\")\n",
    "\n",
    "# Print a summary table\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "print(\"{:<20} {:<10} {:<10} {:<15}\".format(\"Model\", \"Accuracy\", \"Macro F1\", \"Train Time (s)\"))\n",
    "print(\"-\" * 60)\n",
    "for i in range(len(model_names)):\n",
    "    print(\"{:<20} {:<10.4f} {:<10.4f} {:<15.4f}\".format(\n",
    "        model_names[i],\n",
    "        accuracies[i],\n",
    "        macro_f1_scores[i],\n",
    "        train_times[i]\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
